<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Finger + Audio Reactive Visualizer</title>
  <style>
    body {
      margin: 0;
      overflow: hidden;
      background: black;
      font-family: sans-serif;
      color: white;
      text-align: center;
    }
    #controls {
      position: absolute;
      top: 10px;
      left: 50%;
      transform: translateX(-50%);
      z-index: 10;
    }
    video {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      object-fit: cover;
      opacity: 0.3;
      z-index: -1;
    }
    canvas {
      display: block;
    }
    #gestureLabel {
      position: absolute;
      bottom: 20px;
      left: 50%;
      transform: translateX(-50%);
      background: rgba(0, 0, 0, 0.5);
      padding: 8px 16px;
      border-radius: 8px;
      font-size: 18px;
      opacity: 0;
      transition: opacity 0.5s ease;
    }
  </style>
</head>
<body>

<div id="controls">
  <input type="file" id="audioFile" accept="audio/*">
  <button id="stopBtn">Stop Audio & Camera</button>
</div>
<video id="webcam" autoplay playsinline muted></video>
<canvas id="visualizer"></canvas>
<div id="gestureLabel"></div>

<script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>

<script>
let audioCtx, analyser, source, audioBufferSource, dataArray, bufferLength;
let canvas = document.getElementById('visualizer');
let ctx = canvas.getContext('2d');
let videoEl = document.getElementById('webcam');
let gestureLabel = document.getElementById('gestureLabel');
let animationId;
let mediaStream;
let currentGesture = "";
let gestureTimeout;

// Resize canvas
function resizeCanvas() {
  canvas.width = window.innerWidth;
  canvas.height = window.innerHeight;
}
window.addEventListener('resize', resizeCanvas);
resizeCanvas();

// Start camera
async function startCamera() {
  mediaStream = await navigator.mediaDevices.getUserMedia({ video: true });
  videoEl.srcObject = mediaStream;
}
startCamera();

// Audio setup
document.getElementById('audioFile').addEventListener('change', function(e) {
  let file = e.target.files[0];
  if (!file) return;
  let audioURL = URL.createObjectURL(file);

  if (!audioCtx) {
    audioCtx = new (window.AudioContext || window.webkitAudioContext)();
  }

  let audio = new Audio();
  audio.src = audioURL;
  audio.crossOrigin = "anonymous";
  audio.autoplay = true;
  audio.loop = true;

  source = audioCtx.createMediaElementSource(audio);
  analyser = audioCtx.createAnalyser();
  analyser.fftSize = 256;
  bufferLength = analyser.frequencyBinCount;
  dataArray = new Uint8Array(bufferLength);

  source.connect(analyser);
  analyser.connect(audioCtx.destination);

  audio.play();
  draw();
});

document.getElementById('stopBtn').addEventListener('click', () => {
  if (audioCtx) audioCtx.close();
  if (mediaStream) mediaStream.getTracks().forEach(t => t.stop());
  cancelAnimationFrame(animationId);
});

// Draw visualizer
function draw() {
  animationId = requestAnimationFrame(draw);
  analyser.getByteFrequencyData(dataArray);

  ctx.clearRect(0, 0, canvas.width, canvas.height);

  let centerX = canvas.width / 2;
  let centerY = canvas.height / 2;
  let radius = 150;

  ctx.beginPath();
  for (let i = 0; i < bufferLength; i++) {
    let value = dataArray[i];
    let angle = (i / bufferLength) * Math.PI * 2;
    let barLength = value * 0.8;
    let x = centerX + Math.cos(angle) * (radius + barLength);
    let y = centerY + Math.sin(angle) * (radius + barLength);
    ctx.strokeStyle = `hsl(${i * 4}, 100%, 50%)`;
    ctx.moveTo(centerX + Math.cos(angle) * radius, centerY + Math.sin(angle) * radius);
    ctx.lineTo(x, y);
  }
  ctx.stroke();
}

// Hand gesture detection
const hands = new Hands({
  locateFile: file => `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`
});
hands.setOptions({
  maxNumHands: 1,
  minDetectionConfidence: 0.7,
  minTrackingConfidence: 0.7
});
hands.onResults(onResults);

function onResults(results) {
  if (!results.multiHandLandmarks.length) return;

  let landmarks = results.multiHandLandmarks[0];
  let fingerCount = countFingers(landmarks);
  let gestureName = mapFingersToEffect(fingerCount);

  if (gestureName !== currentGesture) {
    currentGesture = gestureName;
    showGestureLabel(gestureName);
  }
}

function countFingers(landmarks) {
  let count = 0;
  const tips = [8, 12, 16, 20]; // Index, Middle, Ring, Pinky
  const pip = [6, 10, 14, 18];
  for (let i = 0; i < tips.length; i++) {
    if (landmarks[tips[i]].y < landmarks[pip[i]].y) count++;
  }
  if (landmarks[4].x < landmarks[3].x) count++; // Thumb
  return count;
}

function mapFingersToEffect(count) {
  switch (count) {
    case 1: return "Effect: Echo";
    case 2: return "Effect: Distortion";
    case 3: return "Effect: Reverb";
    case 4: return "Effect: Filter Sweep";
    case 5: return "Effect: Visual Bloom";
    default: return "Neutral";
  }
}

function showGestureLabel(text) {
  gestureLabel.textContent = text;
  gestureLabel.style.opacity = 1;
  clearTimeout(gestureTimeout);
  gestureTimeout = setTimeout(() => gestureLabel.style.opacity = 0, 2000);
}

// Run camera for hand detection
const cam = new Camera(videoEl, {
  onFrame: async () => {
    await hands.send({ image: videoEl });
  },
  width: 640,
  height: 480
});
cam.start();
</script>
</body>
</html>
