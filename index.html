<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>Finger-Controlled Audio FX</title>
<style>
  body {
    margin: 0;
    background: black;
    color: white;
    font-family: sans-serif;
    overflow: hidden;
  }
  #visualizerCanvas {
    position: absolute;
    top: 0;
    left: 0;
  }
  #controls {
    position: fixed;
    top: 10px;
    left: 10px;
    background: rgba(0,0,0,0.5);
    padding: 10px;
    border-radius: 8px;
  }
  video {
    display: none;
  }
</style>
</head>
<body>
<video id="cameraFeed" autoplay playsinline></video>
<canvas id="visualizerCanvas"></canvas>
<div id="controls">
  <input type="file" id="audioFile" accept="audio/*" />
  <button id="stopBtn">Stop Audio & Camera</button>
</div>

<script type="module">
import {
  Hands,
  HAND_CONNECTIONS
} from "https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js";
import {
  drawConnectors,
  drawLandmarks
} from "https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js";
import {
  Camera
} from "https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js";

const videoEl = document.getElementById('cameraFeed');
const canvas = document.getElementById('visualizerCanvas');
const ctx = canvas.getContext('2d');
canvas.width = window.innerWidth;
canvas.height = window.innerHeight;

let audioCtx, sourceNode, analyser, gainNode, convolver, delayNode, biquadFilter;
let animationId;
let currentEffect = null;

function setupAudioNodes() {
  audioCtx = new AudioContext();
  gainNode = audioCtx.createGain();
  analyser = audioCtx.createAnalyser();
  convolver = audioCtx.createConvolver();
  delayNode = audioCtx.createDelay(5.0);
  biquadFilter = audioCtx.createBiquadFilter();

  gainNode.connect(analyser).connect(audioCtx.destination);
}

function applyEffect(effect) {
  if (!sourceNode) return;
  sourceNode.disconnect();
  switch (effect) {
    case 'reverb':
      sourceNode.connect(convolver).connect(gainNode);
      break;
    case 'echo':
      sourceNode.connect(delayNode).connect(gainNode);
      break;
    case 'lowpass':
      biquadFilter.type = 'lowpass';
      biquadFilter.frequency.value = 800;
      sourceNode.connect(biquadFilter).connect(gainNode);
      break;
    case 'highpass':
      biquadFilter.type = 'highpass';
      biquadFilter.frequency.value = 1200;
      sourceNode.connect(biquadFilter).connect(gainNode);
      break;
    case 'normal':
      sourceNode.connect(gainNode);
      break;
    default:
      sourceNode.connect(gainNode);
  }
  currentEffect = effect;
}

function setPlaybackRate(rate) {
  if (sourceNode && sourceNode.playbackRate) {
    sourceNode.playbackRate.value = rate;
  }
}

document.getElementById('audioFile').addEventListener('change', async (e) => {
  const file = e.target.files[0];
  if (!file) return;
  if (!audioCtx) setupAudioNodes();
  const arrayBuffer = await file.arrayBuffer();
  const audioBuffer = await audioCtx.decodeAudioData(arrayBuffer);
  if (sourceNode) sourceNode.stop();
  sourceNode = audioCtx.createBufferSource();
  sourceNode.buffer = audioBuffer;
  applyEffect(currentEffect || 'normal');
  sourceNode.start();
});

document.getElementById('stopBtn').addEventListener('click', () => {
  if (sourceNode) sourceNode.stop();
  if (camera) camera.stop();
  if (animationId) cancelAnimationFrame(animationId);
});

function drawVisualizer() {
  analyser.fftSize = 256;
  const bufferLength = analyser.frequencyBinCount;
  const dataArray = new Uint8Array(bufferLength);

  function renderFrame() {
    animationId = requestAnimationFrame(renderFrame);

    ctx.globalAlpha = 1.0;
    ctx.drawImage(videoEl, 0, 0, canvas.width, canvas.height);

    ctx.fillStyle = 'rgba(0,0,0,0.4)';
    ctx.fillRect(0, 0, canvas.width, canvas.height);

    analyser.getByteFrequencyData(dataArray);
    const barWidth = (canvas.width / bufferLength) * 2.5;
    let x = 0;
    for (let i = 0; i < bufferLength; i++) {
      const barHeight = dataArray[i];
      ctx.fillStyle = 'hsl(' + (i / bufferLength) * 360 + ',100%,50%)';
      ctx.fillRect(x, canvas.height - barHeight / 2, barWidth, barHeight / 2);
      x += barWidth + 1;
    }
  }
  renderFrame();
}

// Count extended fingers
function countFingers(landmarks) {
  const tips = [8, 12, 16, 20];
  let count = 0;
  // Thumb
  if (landmarks[4].x < landmarks[3].x) count++;
  for (let tip of tips) {
    if (landmarks[tip].y < landmarks[tip - 2].y) count++;
  }
  return count;
}

function detectGesture(landmarks) {
  const fingerCount = countFingers(landmarks);
  if (fingerCount === 0) {
    setPlaybackRate(0.5);
  } else if (fingerCount === 1) {
    applyEffect('reverb');
  } else if (fingerCount === 2) {
    applyEffect('echo');
  } else if (fingerCount === 3) {
    applyEffect('lowpass');
  } else if (fingerCount === 4) {
    applyEffect('highpass');
  } else if (fingerCount === 5) {
    applyEffect('normal');
    setPlaybackRate(1.0);
  }
  // Thumbs up (special case: only thumb extended)
  if (fingerCount === 1 && landmarks[4].y < landmarks[3].y) {
    setPlaybackRate(2.0);
  }
}

const hands = new Hands({
  locateFile: (file) =>
    `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`
});
hands.setOptions({
  maxNumHands: 1,
  modelComplexity: 1,
  minDetectionConfidence: 0.7,
  minTrackingConfidence: 0.7
});
hands.onResults((results) => {
  ctx.clearRect(0, 0, canvas.width, canvas.height);
  ctx.globalAlpha = 0.6;
  ctx.drawImage(results.image, 0, 0, canvas.width, canvas.height);
  ctx.globalAlpha = 1.0;

  if (results.multiHandLandmarks.length > 0) {
    for (const landmarks of results.multiHandLandmarks) {
      drawConnectors(ctx, landmarks, HAND_CONNECTIONS,
        { color: '#0ff', lineWidth: 2 });
      drawLandmarks(ctx, landmarks, { color: '#f0f', lineWidth: 1 });
      detectGesture(landmarks);
    }
  }
});

const camera = new Camera(videoEl, {
  onFrame: async () => {
    await hands.send({ image: videoEl });
  },
  width: 640,
  height: 480
});
camera.start();
drawVisualizer();
</script>
</body>
</html>
